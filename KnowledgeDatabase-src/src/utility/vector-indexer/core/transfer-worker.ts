/**
 * @file æ¬è¿å·¥ä½œå™¨
 * @description å°†æš‚å­˜è¡¨æ•°æ®æ¬è¿åˆ°ç›®æ ‡å‘é‡è¡¨
 */

import type { SurrealClient } from '../db/surreal-client'
import type { GroupedRecords, StagingRecord } from './staging-poller'
import type { IndexerConfig } from '@shared/vector-indexer-ipc.types'

// ============================================================================
// æ—¥å¿—
// ============================================================================

const log = (msg: string, data?: any): void => {
  if (data) {
    console.log(`[TransferWorker] ${msg}`, data)
  } else {
    console.log(`[TransferWorker] ${msg}`)
  }
}

const logError = (msg: string, error?: any): void => {
  console.error(`[TransferWorker] ${msg}`, error)
}

// ============================================================================
// å¸¸é‡
// ============================================================================

/** æš‚å­˜è¡¨æ‰€åœ¨çš„ namespace */
const STAGING_NAMESPACE = 'knowledge'
/** æš‚å­˜è¡¨æ‰€åœ¨çš„ database */
const STAGING_DATABASE = 'system'
/** æš‚å­˜è¡¨å */
const STAGING_TABLE = 'vector_staging'
/** æ¯æ‰¹æ’å…¥çš„è®°å½•æ•° */
const INSERT_BATCH_SIZE = 500

// ============================================================================
// å›è°ƒç±»å‹
// ============================================================================

/** æ–‡æ¡£åµŒå…¥å®Œæˆä¿¡æ¯ */
export interface DocumentEmbeddedInfo {
  targetNamespace: string
  targetDatabase: string
  documentId: string
  fileKey: string
  embeddingConfigId: string
  dimensions: number
  chunkCount: number
}

export interface TransferCallbacks {
  onBatchCompleted?: (tableName: string, count: number, duration: number) => void
  onError?: (message: string, details?: string) => void
  onProgress?: (transferred: number, pending: number, activeTableCount: number) => void
  /** æ–‡æ¡£åµŒå…¥å®Œæˆå›è°ƒï¼ˆæ¯ä¸ªæ–‡æ¡£å®Œæˆåè§¦å‘ï¼‰ */
  onDocumentEmbedded?: (info: DocumentEmbeddedInfo) => void
}

// ============================================================================
// TransferWorker
// ============================================================================

export class TransferWorker {
  private client: SurrealClient
  private config: IndexerConfig
  private callbacks: TransferCallbacks
  private activeTransfers = new Set<string>()
  private tableCache = new Set<string>()
  private totalTransferred = 0

  constructor(client: SurrealClient, config: IndexerConfig, callbacks: TransferCallbacks = {}) {
    this.client = client
    this.config = config
    this.callbacks = callbacks
  }

  // ==========================================================================
  // é…ç½®
  // ==========================================================================

  updateConfig(config: Partial<IndexerConfig>): void {
    this.config = { ...this.config, ...config }
  }

  // ==========================================================================
  // ç»Ÿè®¡
  // ==========================================================================

  getTotalTransferred(): number {
    return this.totalTransferred
  }

  getActiveTableCount(): number {
    return this.activeTransfers.size
  }

  // ==========================================================================
  // æ ¸å¿ƒå¤„ç†
  // ==========================================================================

  /**
   * å¤„ç†åˆ†ç»„åçš„è®°å½•
   * å¹¶è¡Œå¤„ç†æœ€å¤š maxConcurrentTables ä¸ªè¡¨
   */
  async processGroups(groups: GroupedRecords[]): Promise<void> {
    if (groups.length === 0) return

    // é™åˆ¶å¹¶å‘æ•°
    const maxConcurrent = this.config.maxConcurrentTables
    const availableSlots = maxConcurrent - this.activeTransfers.size

    if (availableSlots <= 0) {
      log('All slots occupied, skipping this batch')
      return
    }

    // é€‰æ‹©è¦å¤„ç†çš„åˆ†ç»„ï¼ˆä¼˜å…ˆå¤„ç†è®°å½•æ•°å¤šçš„ï¼‰
    const sortedGroups = [...groups].sort((a, b) => b.records.length - a.records.length)
    const groupsToProcess = sortedGroups
      .filter((g) => !this.activeTransfers.has(g.targetKey))
      .slice(0, availableSlots)

    if (groupsToProcess.length === 0) {
      log('No groups to process (all targets are busy)')
      return
    }

    log(`Processing ${groupsToProcess.length} groups`, {
      tables: groupsToProcess.map((g) => g.tableName),
      totalRecords: groupsToProcess.reduce((sum, g) => sum + g.records.length, 0)
    })

    // å¹¶è¡Œå¤„ç†
    const promises = groupsToProcess.map((group) => this.processGroup(group))
    await Promise.allSettled(promises)
  }

  /**
   * å¤„ç†å•ä¸ªåˆ†ç»„
   */
  private async processGroup(group: GroupedRecords): Promise<void> {
    const { targetKey, namespace, database, tableName, dimensions, records } = group

    // æ ‡è®°ä¸ºæ´»è·ƒ
    this.activeTransfers.add(targetKey)
    const startTime = Date.now()

    try {
      // Step 1: æ ‡è®° processing_started_at
      await this.markProcessingStarted(records.map((r) => r.id))

      // Step 2: ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨
      await this.ensureTargetTable(namespace, database, tableName, dimensions)

      // Step 3: åˆ é™¤å¯èƒ½å†²çªçš„æ—§å‘é‡ï¼ˆæŒ‰ document+chunk_index ç²¾ç¡®åˆ é™¤ï¼‰
      await this.deleteConflictingVectors(namespace, database, tableName, records)

      // Step 4: åˆ†æ‰¹æ’å…¥ç›®æ ‡è¡¨
      await this.insertToTargetTable(namespace, database, tableName, records)

      // Step 5: æ¸…ç†æš‚å­˜è¡¨ï¼ˆå…ˆæ ‡è®°å·²å¤„ç†ï¼Œå†åˆ é™¤ï¼‰
      const recordIds = records.map((r) => r.id)
      await this.markProcessed(recordIds)
      await this.deleteProcessedRecords(recordIds)

      const duration = Date.now() - startTime
      this.totalTransferred += records.length

      log(`Group completed`, {
        tableName,
        count: records.length,
        duration: `${duration}ms`
      })

      this.callbacks.onBatchCompleted?.(tableName, records.length, duration)
      this.callbacks.onProgress?.(
        this.totalTransferred,
        0, // å¾…å¤„ç†æ•°éœ€è¦ä» poller è·å–
        this.activeTransfers.size
      )

      // ğŸ¯ ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£çš„ chunk æ•°é‡å¹¶å‘é€åµŒå…¥å®Œæˆé€šçŸ¥
      this.notifyDocumentsEmbedded(group)
    } catch (error) {
      const msg = error instanceof Error ? error.message : String(error)
      logError(`Group failed: ${tableName}`, msg)
      this.callbacks.onError?.(
        `Transfer failed for ${tableName}`,
        msg
      )
      // ä¸æ ‡è®° processedï¼Œè®©è¶…æ—¶æœºåˆ¶é‡è¯•
    } finally {
      this.activeTransfers.delete(targetKey)
    }
  }

  // ==========================================================================
  // æš‚å­˜è¡¨æ“ä½œ
  // ==========================================================================

  /**
   * æ ‡è®°è®°å½•å¼€å§‹å¤„ç†
   */
  private async markProcessingStarted(ids: string[]): Promise<void> {
    if (ids.length === 0) return

    const sql = `
      UPDATE ${STAGING_TABLE}
      SET processing_started_at = time::now()
      WHERE id IN $ids;
    `

    await this.client.queryInDatabase(
      STAGING_NAMESPACE,
      STAGING_DATABASE,
      sql,
      { ids }
    )
  }

  /**
   * æ ‡è®°è®°å½•å·²å¤„ç†
   * å…ˆæ ‡è®°å†åˆ é™¤ï¼Œç¡®ä¿åˆ é™¤å¤±è´¥æ—¶ä¸ä¼šé‡å¤å¤„ç†
   */
  private async markProcessed(ids: string[]): Promise<void> {
    if (ids.length === 0) return

    const sql = `
      UPDATE ${STAGING_TABLE}
      SET processed = true, processing_started_at = NULL
      WHERE id IN $ids;
    `

    await this.client.queryInDatabase(
      STAGING_NAMESPACE,
      STAGING_DATABASE,
      sql,
      { ids }
    )
  }

  /**
   * åˆ é™¤å·²å¤„ç†çš„æš‚å­˜è®°å½•
   * ç´¢å¼•æˆåŠŸåæ¸…ç†æš‚å­˜è¡¨ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´
   */
  private async deleteProcessedRecords(ids: string[]): Promise<void> {
    if (ids.length === 0) return

    const sql = `
      DELETE FROM ${STAGING_TABLE}
      WHERE id IN $ids;
    `

    try {
      await this.client.queryInDatabase(
        STAGING_NAMESPACE,
        STAGING_DATABASE,
        sql,
        { ids }
      )
      log(`Cleaned up ${ids.length} staging records`)
    } catch (error) {
      // åˆ é™¤å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œè®°å½•è­¦å‘Šå³å¯
      // å› ä¸ºå·²ç»æ ‡è®° processed=trueï¼Œä¸‹æ¬¡è½®è¯¢ä¸ä¼šé‡å¤å¤„ç†
      const msg = error instanceof Error ? error.message : String(error)
      console.warn(`[TransferWorker] Failed to cleanup staging records: ${msg}`)
    }
  }

  // ==========================================================================
  // ç›®æ ‡è¡¨æ“ä½œ
  // ==========================================================================

  /**
   * åˆ é™¤å¯èƒ½å†²çªçš„æ—§å‘é‡
   * ç²¾ç¡®æŒ‰ (document, chunk_index) ç»„åˆåˆ é™¤ï¼Œé¿å…è¯¯åˆ å…¶ä»–æ‰¹æ¬¡å·²æ’å…¥çš„æ•°æ®
   */
  private async deleteConflictingVectors(
    namespace: string,
    database: string,
    tableName: string,
    records: StagingRecord[]
  ): Promise<void> {
    if (records.length === 0) return

    // æ„å»ºå¾…åˆ é™¤çš„ (document, chunk_index) ç»„åˆ
    const pairs = records.map((r) => [r.document_id, r.chunk_index])

    log(`Deleting ${pairs.length} potentially conflicting vectors in ${tableName}`)

    // æ‰¹é‡åˆ é™¤ï¼šä½¿ç”¨ OR æ¡ä»¶åŒ¹é…æ¯ä¸ª (document, chunk_index) ç»„åˆ
    // åˆ†æ‰¹å¤„ç†ä»¥é¿å… SQL è¿‡é•¿
    const BATCH_SIZE = 100
    for (let i = 0; i < pairs.length; i += BATCH_SIZE) {
      const batch = pairs.slice(i, i + BATCH_SIZE)
      const conditions = batch
        .map((_, idx) => `(document = $doc${idx} AND chunk_index = $idx${idx})`)
        .join(' OR ')

      const params: Record<string, any> = {}
      batch.forEach(([doc, idx], i) => {
        params[`doc${i}`] = doc
        params[`idx${i}`] = idx
      })

      const sql = `DELETE FROM \`${tableName}\` WHERE ${conditions};`

      try {
        await this.client.queryInDatabase(namespace, database, sql, params)
      } catch (error) {
        const msg = error instanceof Error ? error.message : String(error)
        // å¦‚æœè¡¨ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡åˆ›å»ºï¼‰ï¼Œå¿½ç•¥åˆ é™¤é”™è¯¯
        if (!msg.includes('not found') && !msg.includes('does not exist')) {
          logError(`Failed to delete conflicting vectors from ${tableName}`, msg)
          // ä¸æŠ›å‡ºé”™è¯¯ï¼Œç»§ç»­å°è¯•æ’å…¥
        }
      }
    }
  }

  /**
   * ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨ï¼ˆå« HNSW ç´¢å¼•ï¼‰
   */
  private async ensureTargetTable(
    namespace: string,
    database: string,
    tableName: string,
    dimensions: number
  ): Promise<void> {
    const cacheKey = `${namespace}.${database}.${tableName}`
    if (this.tableCache.has(cacheKey)) {
      return
    }

    const sql = `
      DEFINE TABLE IF NOT EXISTS \`${tableName}\`;
      DEFINE INDEX IF NOT EXISTS uniq_doc_chunk
        ON TABLE \`${tableName}\` FIELDS document, chunk_index UNIQUE;
      DEFINE INDEX IF NOT EXISTS hnsw_embedding
        ON TABLE \`${tableName}\` FIELDS embedding
        HNSW DIMENSION ${dimensions} DIST COSINE TYPE F32 EFC 200 M 16;
    `

    try {
      await this.client.queryInDatabase(namespace, database, sql)
      this.tableCache.add(cacheKey)
      log(`Ensured target table: ${tableName}`)
    } catch (error) {
      const msg = error instanceof Error ? error.message : String(error)
      // å¦‚æœè¡¨/ç´¢å¼•å·²å­˜åœ¨ï¼Œè§†ä¸ºæˆåŠŸ
      if (!msg.includes('already exists')) {
        throw error
      }
      this.tableCache.add(cacheKey)
    }
  }

  /**
   * åˆ†æ‰¹æ’å…¥ç›®æ ‡è¡¨
   * ä½¿ç”¨ UPSERT ä¿è¯å¹‚ç­‰æ€§
   */
  private async insertToTargetTable(
    namespace: string,
    database: string,
    tableName: string,
    records: StagingRecord[]
  ): Promise<void> {
    const totalBatches = Math.ceil(records.length / INSERT_BATCH_SIZE)

    for (let i = 0; i < totalBatches; i++) {
      const start = i * INSERT_BATCH_SIZE
      const end = Math.min(start + INSERT_BATCH_SIZE, records.length)
      const batch = records.slice(start, end)

      // æ„å»ºæ‰¹é‡ UPSERT æ•°æ®
      const chunkData = batch.map((record) => ({
        document: record.document_id,
        chunk_index: record.chunk_index,
        content: record.content,
        char_count: record.char_count,
        start_char: record.start_char,
        end_char: record.end_char,
        embedding: record.embedding,
        file_key: record.file_key,
        file_name: record.file_name
      }))

      // ä½¿ç”¨ INSERTï¼ˆå·²åˆ é™¤å†²çªæ•°æ®ï¼‰
      const sql = `INSERT INTO \`${tableName}\` $chunks;`
      await this.client.queryInDatabase(namespace, database, sql, { chunks: chunkData })
    }
  }

  // ==========================================================================
  // æ–‡æ¡£åµŒå…¥å®Œæˆé€šçŸ¥
  // ==========================================================================

  /**
   * ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£çš„ chunk æ•°é‡å¹¶å‘é€åµŒå…¥å®Œæˆé€šçŸ¥
   */
  private notifyDocumentsEmbedded(group: GroupedRecords): void {
    if (!this.callbacks.onDocumentEmbedded) return

    // æŒ‰æ–‡æ¡£åˆ†ç»„ç»Ÿè®¡ chunk æ•°é‡
    const documentChunkCounts = new Map<string, {
      fileKey: string
      chunkCount: number
    }>()

    for (const record of group.records) {
      const existing = documentChunkCounts.get(record.document_id)
      if (existing) {
        existing.chunkCount++
      } else {
        documentChunkCounts.set(record.document_id, {
          fileKey: record.file_key,
          chunkCount: 1
        })
      }
    }

    // ä¸ºæ¯ä¸ªæ–‡æ¡£å‘é€åµŒå…¥å®Œæˆé€šçŸ¥
    for (const [documentId, info] of documentChunkCounts) {
      this.callbacks.onDocumentEmbedded({
        targetNamespace: group.namespace,
        targetDatabase: group.database,
        documentId,
        fileKey: info.fileKey,
        embeddingConfigId: group.embeddingConfigId,
        dimensions: group.dimensions,
        chunkCount: info.chunkCount
      })
    }
  }
}
