/**
 * @file æ¬è¿å·¥ä½œå™¨
 * @description å°†æš‚å­˜è¡¨æ•°æ®æ¬è¿åˆ°ç›®æ ‡å‘é‡è¡¨
 */

import type { SurrealClient } from '../db/surreal-client'
import type { GroupedRecords, StagingRecord } from './staging-poller'
import type { IndexerConfig } from '@shared/vector-indexer-ipc.types'

// ============================================================================
// æ—¥å¿—
// ============================================================================

const log = (msg: string, data?: any): void => {
  if (data) {
    console.log(`[TransferWorker] ${msg}`, data)
  } else {
    console.log(`[TransferWorker] ${msg}`)
  }
}

const logError = (msg: string, error?: any): void => {
  console.error(`[TransferWorker] ${msg}`, error)
}

// ============================================================================
// å¸¸é‡
// ============================================================================

/** æš‚å­˜è¡¨æ‰€åœ¨çš„ namespace */
const STAGING_NAMESPACE = 'knowledge'
/** æš‚å­˜è¡¨æ‰€åœ¨çš„ database */
const STAGING_DATABASE = 'system'
/** æš‚å­˜è¡¨å */
const STAGING_TABLE = 'vector_staging'
/** æ¯æ‰¹æ’å…¥çš„è®°å½•æ•° */
const INSERT_BATCH_SIZE = 500

// ============================================================================
// å›è°ƒç±»å‹
// ============================================================================

/** æ–‡æ¡£åµŒå…¥å®Œæˆä¿¡æ¯ */
export interface DocumentEmbeddedInfo {
  targetNamespace: string
  targetDatabase: string
  documentId: string
  fileKey: string
  embeddingConfigId: string
  dimensions: number
  chunkCount: number
}

export interface TransferCallbacks {
  onBatchCompleted?: (tableName: string, count: number, duration: number) => void
  onError?: (message: string, details?: string) => void
  onProgress?: (transferred: number, pending: number, activeTableCount: number) => void
  /** æ–‡æ¡£åµŒå…¥å®Œæˆå›è°ƒï¼ˆæ¯ä¸ªæ–‡æ¡£å®Œæˆåè§¦å‘ï¼‰ */
  onDocumentEmbedded?: (info: DocumentEmbeddedInfo) => void
}

// ============================================================================
// TransferWorker
// ============================================================================

export class TransferWorker {
  private client: SurrealClient
  private config: IndexerConfig
  private callbacks: TransferCallbacks
  private activeTransfers = new Set<string>()
  private tableCache = new Set<string>()
  private totalTransferred = 0

  constructor(client: SurrealClient, config: IndexerConfig, callbacks: TransferCallbacks = {}) {
    this.client = client
    this.config = config
    this.callbacks = callbacks
  }

  // ==========================================================================
  // é…ç½®
  // ==========================================================================

  updateConfig(config: Partial<IndexerConfig>): void {
    this.config = { ...this.config, ...config }
  }

  // ==========================================================================
  // ç»Ÿè®¡
  // ==========================================================================

  getTotalTransferred(): number {
    return this.totalTransferred
  }

  getActiveTableCount(): number {
    return this.activeTransfers.size
  }

  // ==========================================================================
  // æ ¸å¿ƒå¤„ç†
  // ==========================================================================

  /**
   * å¤„ç†åˆ†ç»„åçš„è®°å½•
   * ä¸²è¡Œå¤„ç†è®°å½•æ•°æœ€å¤šçš„ 1 ä¸ª groupï¼ˆâ‰¤batchSize æ¡ï¼‰
   */
  async processGroups(groups: GroupedRecords[]): Promise<void> {
    if (groups.length === 0) return

    // æŒ‰è®°å½•æ•°é™åºï¼Œå–æœ€å¤§çš„ group
    const sorted = [...groups].sort((a, b) => b.records.length - a.records.length)
    const biggest = sorted[0]

    // æœ€å¤šå– batchSize æ¡ï¼ˆé»˜è®¤ 100ï¼‰
    const limit = this.config.batchSize
    const records = biggest.records.slice(0, limit)

    const subGroup: GroupedRecords = {
      ...biggest,
      records
    }

    log(`Processing group`, {
      table: subGroup.tableName,
      records: records.length,
      totalPending: biggest.records.length
    })

    await this.processGroup(subGroup)
  }

  /**
   * å¤„ç†å•ä¸ªåˆ†ç»„
   */
  private async processGroup(group: GroupedRecords): Promise<void> {
    const { targetKey, namespace, database, tableName, dimensions, records } = group

    // æ ‡è®°ä¸ºæ´»è·ƒ
    this.activeTransfers.add(targetKey)
    const startTime = Date.now()

    try {
      // Step 1: æ ‡è®° processing_started_at
      await this.markProcessingStarted(records.map((r) => r.id))

      // Step 2: ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨
      await this.ensureTargetTable(namespace, database, tableName, dimensions)

      // Step 3: åˆ é™¤å¯èƒ½å†²çªçš„æ—§å‘é‡ï¼ˆæŒ‰ document+chunk_index ç²¾ç¡®åˆ é™¤ï¼‰
      await this.deleteConflictingVectors(namespace, database, tableName, records)

      // Step 4: åˆ†æ‰¹æ’å…¥ç›®æ ‡è¡¨
      await this.insertToTargetTable(namespace, database, tableName, records)

      // Step 5: æ ‡è®°å·²å¤„ç†ï¼ˆä¸ç«‹å³åˆ é™¤ï¼Œç•™ç»™ idle æ—¶æ‰¹é‡æ¸…ç†ï¼‰
      const recordIds = records.map((r) => r.id)
      await this.markProcessed(recordIds)

      const duration = Date.now() - startTime
      this.totalTransferred += records.length

      log(`Group completed`, {
        tableName,
        count: records.length,
        duration: `${duration}ms`
      })

      this.callbacks.onBatchCompleted?.(tableName, records.length, duration)
      this.callbacks.onProgress?.(
        this.totalTransferred,
        0, // å¾…å¤„ç†æ•°éœ€è¦ä» poller è·å–
        this.activeTransfers.size
      )

      // ğŸ¯ ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£çš„ chunk æ•°é‡å¹¶å‘é€åµŒå…¥å®Œæˆé€šçŸ¥
      this.notifyDocumentsEmbedded(group)
    } catch (error) {
      const msg = error instanceof Error ? error.message : String(error)
      logError(`Group failed: ${tableName}`, msg)
      this.callbacks.onError?.(`Transfer failed for ${tableName}`, msg)
      // ä¸æ ‡è®° processedï¼Œè®©è¶…æ—¶æœºåˆ¶é‡è¯•
    } finally {
      this.activeTransfers.delete(targetKey)
    }
  }

  // ==========================================================================
  // æš‚å­˜è¡¨æ“ä½œ
  // ==========================================================================

  /**
   * æ ‡è®°è®°å½•å¼€å§‹å¤„ç†
   */
  private async markProcessingStarted(ids: string[]): Promise<void> {
    if (ids.length === 0) return

    const sql = `
      UPDATE ${STAGING_TABLE}
      SET processing_started_at = time::now()
      WHERE id IN $ids
      RETURN NONE;
    `

    await this.client.queryInDatabase(STAGING_NAMESPACE, STAGING_DATABASE, sql, { ids })
  }

  /**
   * æ ‡è®°è®°å½•å·²å¤„ç†
   * å…ˆæ ‡è®°å†åˆ é™¤ï¼Œç¡®ä¿åˆ é™¤å¤±è´¥æ—¶ä¸ä¼šé‡å¤å¤„ç†
   */
  private async markProcessed(ids: string[]): Promise<void> {
    if (ids.length === 0) return

    const sql = `
      UPDATE ${STAGING_TABLE}
      SET processed = true, processing_started_at = NULL
      WHERE id IN $ids
      RETURN NONE;
    `

    await this.client.queryInDatabase(STAGING_NAMESPACE, STAGING_DATABASE, sql, { ids })
  }

  // ==========================================================================
  // ç›®æ ‡è¡¨æ“ä½œ
  // ==========================================================================

  /**
   * åˆ é™¤å¯èƒ½å†²çªçš„æ—§å‘é‡
   * ç²¾ç¡®æŒ‰ (document, chunk_index) ç»„åˆåˆ é™¤ï¼Œé¿å…è¯¯åˆ å…¶ä»–æ‰¹æ¬¡å·²æ’å…¥çš„æ•°æ®
   */
  private async deleteConflictingVectors(
    namespace: string,
    database: string,
    tableName: string,
    records: StagingRecord[]
  ): Promise<void> {
    if (records.length === 0) return

    // æ„å»ºå¾…åˆ é™¤çš„ (document, chunk_index) ç»„åˆ
    const pairs = records.map((r) => [r.document_id, r.chunk_index])

    log(`Deleting ${pairs.length} potentially conflicting vectors in ${tableName}`)

    // æ‰¹é‡åˆ é™¤ï¼šä½¿ç”¨ OR æ¡ä»¶åŒ¹é…æ¯ä¸ª (document, chunk_index) ç»„åˆ
    // åˆ†æ‰¹å¤„ç†ä»¥é¿å… SQL è¿‡é•¿
    const BATCH_SIZE = 100
    for (let i = 0; i < pairs.length; i += BATCH_SIZE) {
      const batch = pairs.slice(i, i + BATCH_SIZE)
      const conditions = batch
        .map((_, idx) => `(document = $doc${idx} AND chunk_index = $idx${idx})`)
        .join(' OR ')

      const params: Record<string, any> = {}
      batch.forEach(([doc, idx], i) => {
        params[`doc${i}`] = doc
        params[`idx${i}`] = idx
      })

      const sql = `DELETE FROM \`${tableName}\` WHERE ${conditions} RETURN NONE;`

      try {
        await this.client.queryInDatabase(namespace, database, sql, params)
      } catch (error) {
        const msg = error instanceof Error ? error.message : String(error)
        // å¦‚æœè¡¨ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡åˆ›å»ºï¼‰ï¼Œå¿½ç•¥åˆ é™¤é”™è¯¯
        if (!msg.includes('not found') && !msg.includes('does not exist')) {
          logError(`Failed to delete conflicting vectors from ${tableName}`, msg)
          // ä¸æŠ›å‡ºé”™è¯¯ï¼Œç»§ç»­å°è¯•æ’å…¥
        }
      }
    }
  }

  /**
   * ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨ï¼ˆå« HNSW ç´¢å¼•ï¼‰
   */
  private async ensureTargetTable(
    namespace: string,
    database: string,
    tableName: string,
    dimensions: number
  ): Promise<void> {
    const cacheKey = `${namespace}.${database}.${tableName}`
    if (this.tableCache.has(cacheKey)) {
      return
    }

    const sql = `
      DEFINE TABLE IF NOT EXISTS \`${tableName}\`;
      DEFINE INDEX IF NOT EXISTS uniq_doc_chunk
        ON TABLE \`${tableName}\` FIELDS document, chunk_index UNIQUE;
      DEFINE INDEX IF NOT EXISTS hnsw_embedding
        ON TABLE \`${tableName}\` FIELDS embedding
        HNSW DIMENSION ${dimensions} DIST COSINE TYPE F32 EFC 200 M 16;
    `

    try {
      await this.client.queryInDatabase(namespace, database, sql)
      this.tableCache.add(cacheKey)
      log(`Ensured target table: ${tableName}`)
    } catch (error) {
      const msg = error instanceof Error ? error.message : String(error)
      // å¦‚æœè¡¨/ç´¢å¼•å·²å­˜åœ¨ï¼Œè§†ä¸ºæˆåŠŸ
      if (!msg.includes('already exists')) {
        throw error
      }
      this.tableCache.add(cacheKey)
    }
  }

  /**
   * åˆ†æ‰¹æ’å…¥ç›®æ ‡è¡¨
   * ä½¿ç”¨ UPSERT ä¿è¯å¹‚ç­‰æ€§
   */
  private async insertToTargetTable(
    namespace: string,
    database: string,
    tableName: string,
    records: StagingRecord[]
  ): Promise<void> {
    const totalBatches = Math.ceil(records.length / INSERT_BATCH_SIZE)

    for (let i = 0; i < totalBatches; i++) {
      const start = i * INSERT_BATCH_SIZE
      const end = Math.min(start + INSERT_BATCH_SIZE, records.length)
      const batch = records.slice(start, end)

      // æ„å»ºæ‰¹é‡ UPSERT æ•°æ®
      const chunkData = batch.map((record) => ({
        document: record.document_id,
        chunk_index: record.chunk_index,
        content: record.content,
        char_count: record.char_count,
        start_char: record.start_char,
        end_char: record.end_char,
        embedding: record.embedding,
        file_key: record.file_key,
        file_name: record.file_name
      }))

      // ä½¿ç”¨ INSERTï¼ˆå·²åˆ é™¤å†²çªæ•°æ®ï¼‰
      const sql = `INSERT INTO \`${tableName}\` $chunks;`
      await this.client.queryInDatabase(namespace, database, sql, { chunks: chunkData })
    }
  }

  // ==========================================================================
  // æ–‡æ¡£åµŒå…¥å®Œæˆé€šçŸ¥
  // ==========================================================================

  /**
   * ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£çš„ chunk æ•°é‡å¹¶å‘é€åµŒå…¥å®Œæˆé€šçŸ¥
   */
  private notifyDocumentsEmbedded(group: GroupedRecords): void {
    if (!this.callbacks.onDocumentEmbedded) return

    // æŒ‰æ–‡æ¡£åˆ†ç»„ç»Ÿè®¡ chunk æ•°é‡
    const documentChunkCounts = new Map<
      string,
      {
        fileKey: string
        chunkCount: number
      }
    >()

    for (const record of group.records) {
      const existing = documentChunkCounts.get(record.document_id)
      if (existing) {
        existing.chunkCount++
      } else {
        documentChunkCounts.set(record.document_id, {
          fileKey: record.file_key,
          chunkCount: 1
        })
      }
    }

    // ä¸ºæ¯ä¸ªæ–‡æ¡£å‘é€åµŒå…¥å®Œæˆé€šçŸ¥
    for (const [documentId, info] of documentChunkCounts) {
      this.callbacks.onDocumentEmbedded({
        targetNamespace: group.namespace,
        targetDatabase: group.database,
        documentId,
        fileKey: info.fileKey,
        embeddingConfigId: group.embeddingConfigId,
        dimensions: group.dimensions,
        chunkCount: info.chunkCount
      })
    }
  }
}
